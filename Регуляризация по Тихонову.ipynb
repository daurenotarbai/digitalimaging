{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:100: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:104: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:100: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:104: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<ipython-input-1-81d85903d5ea>:100: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if ko.shape is ():  # there is no lower part of matrix\n",
      "<ipython-input-1-81d85903d5ea>:104: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if hq.shape is ():  # special case where L is square\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Tikhonov regression,\n",
    "based on implementation by Stout and Kalivas, 2006. Journal of Chemometrics\n",
    "L2-regularized regression using a non-diagonal regularization matrix\n",
    "This can be done in two ways, by setting the original problem into\n",
    "\"standard space\", such that regular ridge regression can be employed,\n",
    "or solving the equation in original space. As number features increases,\n",
    "rotating the original problem should be faster\n",
    "\"\"\"\n",
    "# Author: Jeff Chiang <jeff.njchiang@gmail.com>\n",
    "# License: BSD 3 clause\n",
    "import numpy as np\n",
    "from scipy.linalg import solve_triangular\n",
    "from sklearn.linear_model import Ridge, RidgeCV\n",
    "from sklearn.utils import check_X_y\n",
    "# from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "\n",
    "# QR method\n",
    "def _qr(x):\n",
    "    m, n = x.shape\n",
    "    q, r = np.linalg.qr(x, mode='complete')\n",
    "    rp = r[:n, :]\n",
    "    qp = q[:, :n]\n",
    "    qo = q[:, n:]\n",
    "    # check for degenerate case\n",
    "    if qo.shape[1] == 0:\n",
    "        qo = np.array(1.0)\n",
    "    return qp, qo, rp\n",
    "\n",
    "\n",
    "def analytic_tikhonov(x, y, alpha, sigma=None):\n",
    "    \"\"\"\n",
    "    Solves tikhonov regularization problem with the covariance of the\n",
    "    weights as a prior\n",
    "    b = inv(X.T*X + inv(Sigma)*alpha) * X.T * y\n",
    "    :param x: {array-like},\n",
    "        shape = [n_samples, n_features]\n",
    "        Training data\n",
    "    :param y: array-like, shape = [n_samples] or [n_samples, n_targets]\n",
    "        Target values\n",
    "    :param alpha: scalar, regularization parameter\n",
    "    :param sigma: array-like, shape = [n_features, n_features].\n",
    "        Covariance matrix of the prior\n",
    "    :return:\n",
    "    beta_hat: beta weight estimates\n",
    "    \"\"\"\n",
    "    if sigma is None:\n",
    "        sigma = np.eye(x.shape[1])\n",
    "    return np.dot(np.linalg.pinv(np.dot(x.T, x) +\n",
    "                  np.linalg.pinv(sigma) *\n",
    "                  alpha), np.dot(x.T, y))\n",
    "\n",
    "\n",
    "def find_tikhonov_from_covariance(x, cutoff=.0001, eps=1e-10):\n",
    "    \"\"\"\n",
    "    Use truncated-SVD to find Tikhonov matrix\n",
    "    :param x: feature x feature covariance matrix. This is used to\n",
    "    find a Tikhonov matrix L such that:\n",
    "     inv(x) = L.T * L\n",
    "    :param cutoff: cutoff value for singular value magnitude. if it's too low,\n",
    "    rank will suffer.\n",
    "    :return: L: the Tikhonov matrix for this situation\n",
    "    \"\"\"\n",
    "    if not np.allclose(x.T, x):\n",
    "        raise ValueError(\"Input matrix is not symmetric. \"\n",
    "                         \"Are you sure it is covariance?\")\n",
    "    _, s, vt = np.linalg.svd(x)\n",
    "    return np.dot(np.diag(1/np.sqrt(s[s > cutoff])), vt[s > cutoff])\n",
    "    # _, s, vt = np.linalg.svd(np.linalg.pinv(x))\n",
    "    # return np.dot(np.diag(np.sqrt(s[s > cutoff])), vt[s > cutoff])\n",
    "    # return np.linalg.cholesky(np.linalg.pinv(x)).T\n",
    "    # _, s, vh = np.linalg.svd(x-x.mean(0), full_matrices=False)\n",
    "    # return np.dot(np.diag(1/s[s > cutoff]), vh[s > cutoff, :])\n",
    "\n",
    "\n",
    "def _standardize_params(x, L):\n",
    "    \"\"\"\n",
    "    Calculates parameters associated with rotating the data to standard form\n",
    "    :param x: {array-like},\n",
    "        shape = [n_samples, n_features]\n",
    "        Training data    b = inv(X.T*X + inv(Sigma)*alpha) * X.T * y\n",
    "    :param L: array-like, shape = [n_features, n_regularizers].\n",
    "        Tikhonov matrix\n",
    "    returns:\n",
    "        hq: array-like\n",
    "        kp: array-like, shape = [n_samples] or [n_samples, n_targets]\n",
    "            first block matrix of QR factorization of L.T.\n",
    "            kp * rp^-1.T is inv(L)\n",
    "        rp: array-like, shape = [n_samples] or [n_samples, n_targets]\n",
    "            upper triangular matrix of QR factorization of L.T\n",
    "        ko: array-like, shape = [n_samples] or [n_samples, n_targets]\n",
    "            Target values\n",
    "        to: array-like, shape = [n_samples] or [n_samples, n_targets]\n",
    "            Target values\n",
    "        ho: array-like, shape = [n_samples] or [n_samples, n_targets]\n",
    "            Target values\n",
    "    \"\"\"\n",
    "    kp, ko, rp = _qr(L.T)\n",
    "    if ko.shape is ():  # there is no lower part of matrix\n",
    "        ho, hq, to = np.array(1.0), np.array(1.0), np.array(1.0)\n",
    "    else:\n",
    "        ho, hq, to = _qr(np.dot(x, ko))\n",
    "    if hq.shape is ():  # special case where L is square\n",
    "                        # (saves computational time later\n",
    "        ko, to, ho = None, None, None\n",
    "    return hq, kp, rp, ko, ho, to\n",
    "\n",
    "\n",
    "def to_standard_form(x, y, L):\n",
    "    \"\"\"\n",
    "    Converts x and y into \"standard form\" in order to efficiently\n",
    "    solve the Tikhonov regression problem.\n",
    "    gamma is the Tikhonov regularizer, such that L.T * L can be the inverse\n",
    "    covariance matrix of the data.\n",
    "    :param x: {array-like},\n",
    "        shape = [n_samples, n_features]\n",
    "        Training data\n",
    "    :param y: array-like, shape = [n_samples] or [n_samples, n_targets]\n",
    "        Target values\n",
    "    :param L: array-like, shape = [n_features, n_regularizers]\n",
    "        Generally, L.T * L is the inverse covariance matrix of the data.\n",
    "    :return:\n",
    "    x_new : {array-like}, transformed x\n",
    "    y_new : {array-like}, transformed y\n",
    "    \"\"\"\n",
    "    hq, kp, rp, _, _, _ = _standardize_params(x, L)\n",
    "    # this is derived by doing a bit of algebra:\n",
    "    # x_new = hq.T * x * kp * inv(rp).T\n",
    "    x_new = solve_triangular(rp, np.dot(kp.T, np.dot(x.T, hq))).T\n",
    "    y_new = np.dot(hq.T, y)\n",
    "    return x_new, y_new\n",
    "\n",
    "\n",
    "def to_general_form(b, x, y, L):\n",
    "    \"\"\"\n",
    "    Converts weights back into general form space.\n",
    "    :param x: {array-like},\n",
    "        shape = [n_samples, n_features]\n",
    "        Training data\n",
    "    :param y: array-like, shape = [n_samples] or [n_samples, n_targets]\n",
    "        Target values\n",
    "    :param b: array-like, shape = [n_features] or [n_features, n_targets]\n",
    "        regression coefficients\n",
    "    :param L: arra-like, shape = [n_features, n_regularizers]\n",
    "        Generally, L.T* L is the inverse covariance matrix of the data\n",
    "    :return:\n",
    "    b : ridge coefficients rotated back to original space\n",
    "    \"\"\"\n",
    "    hq, kp, rp, ko, ho, to = _standardize_params(x, L)\n",
    "\n",
    "    if ko is to is ho is None:\n",
    "        L_inv = np.dot(kp, np.linalg.pinv(rp.T))\n",
    "        return np.dot(L_inv, b)\n",
    "        # return np.linalg.solve(np.dot(rp.T, kp.T), b)\n",
    "    else:\n",
    "        L_inv = np.dot(kp, np.linalg.pinv(rp.T))\n",
    "        kth = np.dot(ko, np.dot(np.linalg.pinv(to), ho.T))\n",
    "        resid = y - np.dot(x, np.dot(L_inv, b))\n",
    "        # kth and resid should be 0...\n",
    "        return np.dot(L_inv, b) + np.dot(kth, resid)\n",
    "\n",
    "\n",
    "def fit_learner(x, y, L, ridge=None):\n",
    "    \"\"\"\n",
    "    Returns an trained model that works exactly the same as Ridge,\n",
    "    but fit optimally\n",
    "    \"\"\"\n",
    "    if ridge is None:\n",
    "        ridge = Ridge(fit_intercept=False)\n",
    "    x_new, y_new = to_standard_form(x, y, L)\n",
    "    ta_est_standard = ridge.fit(x_new, y_new).coef_\n",
    "    ta_est = to_general_form(ta_est_standard.T, x, y, L)\n",
    "    ridge.coef_ = ta_est.T\n",
    "    return ridge\n",
    "\n",
    "\n",
    "class Tikhonov(Ridge):\n",
    "\n",
    "    def __init__(self, alpha=1.0, fit_intercept=True, normalize=False,\n",
    "                 copy_X=True, max_iter=None, tol=1e-3, solver=\"auto\",\n",
    "                 random_state=None):\n",
    "        super(Ridge, self).__init__(alpha=alpha, fit_intercept=fit_intercept,\n",
    "                                    normalize=normalize, copy_X=copy_X,\n",
    "                                    max_iter=max_iter, tol=tol, solver=solver,\n",
    "                                    random_state=random_state)\n",
    "\n",
    "    def fit(self, X, y, L=None, sample_weight=None):\n",
    "        if self.solver in ('sag', 'saga'):\n",
    "            _dtype = np.float64\n",
    "        else:\n",
    "            # all other solvers work at both float precision levels\n",
    "            _dtype = [np.float64, np.float32]\n",
    "\n",
    "        X, y = check_X_y(X, y, ['csr', 'csc', 'coo'], dtype=_dtype,\n",
    "                         multi_output=True, y_numeric=True)\n",
    "\n",
    "        if ((sample_weight is not None) and\n",
    "                    np.atleast_1d(sample_weight).ndim > 1):\n",
    "            raise ValueError(\"Sample weights must be 1D array or scalar\")\n",
    "\n",
    "        X, y, X_offset, y_offset, X_scale = self._preprocess_data(\n",
    "            X, y, self.fit_intercept, self.normalize, self.copy_X,\n",
    "            sample_weight=sample_weight)\n",
    "\n",
    "        if L is not None:\n",
    "            x_new, y_new = to_standard_form(X, y, L)\n",
    "            model = super(Ridge, self)\n",
    "            model.fit(x_new, y_new, sample_weight=sample_weight)\n",
    "            standard_coefs = to_general_form(self.coef_.T, X, y, L)\n",
    "            self.coef_ = standard_coefs.T\n",
    "        else:\n",
    "            super(Ridge, self).fit(X, y, sample_weight=sample_weight)\n",
    "\n",
    "        self._set_intercept(X_offset, y_offset, X_scale)\n",
    "        return self\n",
    "\n",
    "\n",
    "class TikhonovCV(RidgeCV):\n",
    "\n",
    "    def __init__(self, alphas=(0.1, 1.0, 10.0),\n",
    "                 fit_intercept=True, normalize=False, scoring=None, copy_X=True,\n",
    "                 cv=None, gcv_mode=None,\n",
    "                 store_cv_values=False):\n",
    "        self.alphas = alphas\n",
    "        self.fit_intercept = fit_intercept\n",
    "        self.normalize = normalize\n",
    "        self.scoring = scoring\n",
    "        self.cv = cv\n",
    "        self.gcv_mode = gcv_mode\n",
    "        self.store_cv_values = store_cv_values\n",
    "        self.copy_X = copy_X\n",
    "\n",
    "    def fit(self, X, y, L=None, sample_weight=None):\n",
    "\n",
    "        X, y = check_X_y(X, y, ['csr', 'csc', 'coo'], dtype=np.float64,\n",
    "                         multi_output=True, y_numeric=True)\n",
    "\n",
    "        if ((sample_weight is not None) and\n",
    "                    np.atleast_1d(sample_weight).ndim > 1):\n",
    "            raise ValueError(\"Sample weights must be 1D array or scalar\")\n",
    "\n",
    "        X, y, X_offset, y_offset, X_scale = self._preprocess_data(\n",
    "            X, y, self.fit_intercept, self.normalize, self.copy_X,\n",
    "            sample_weight=sample_weight)\n",
    "\n",
    "        if L is not None:\n",
    "            x_new, y_new = to_standard_form(X, y, L)\n",
    "            model = super(RidgeCV, self)\n",
    "            model.fit(x_new, y_new, sample_weight=sample_weight)\n",
    "            standard_coefs = to_general_form(self.coef_.T, X, y, L)\n",
    "            self.coef_ = standard_coefs.T\n",
    "        else:\n",
    "            super(RidgeCV, self).fit(X, y, sample_weight=sample_weight)\n",
    "\n",
    "        self._set_intercept(X_offset, y_offset, X_scale)\n",
    "        return self\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
